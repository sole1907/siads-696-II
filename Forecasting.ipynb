{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a475ebb4",
   "metadata": {},
   "source": [
    "__Forecasting__\n",
    "\n",
    "- Build sequences (input windows).\n",
    "- Train baselines (naive, EWMA).\n",
    "- Train LSTM/GRU\n",
    "- Evaluate per horizon and by regime; rolling origin backtest.\n",
    "- Refine with regime‑aware heads or Seq2Seq if needed.\n",
    "- Stability: Try walk‑forward validation to ensure the improvement holds across different time periods/regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405f7333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 08:58:52.861656: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras import layers, models\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e955da",
   "metadata": {},
   "source": [
    "__Load parquet with aggregated features and regimes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4416fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ATM_IV</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Curvature</th>\n",
       "      <th>treasury_2y</th>\n",
       "      <th>ticker</th>\n",
       "      <th>year</th>\n",
       "      <th>Regime_K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>0.282223</td>\n",
       "      <td>0.047618</td>\n",
       "      <td>-0.000384</td>\n",
       "      <td>4.40</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>0.274456</td>\n",
       "      <td>0.061536</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>4.36</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>0.282380</td>\n",
       "      <td>0.038932</td>\n",
       "      <td>-0.006200</td>\n",
       "      <td>4.45</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>0.258931</td>\n",
       "      <td>0.052242</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>4.24</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>0.267719</td>\n",
       "      <td>0.045165</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>4.19</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    ATM_IV      Skew  Curvature  treasury_2y ticker  year  \\\n",
       "0 2023-01-03  0.282223  0.047618  -0.000384         4.40    QQQ  2023   \n",
       "1 2023-01-04  0.274456  0.061536  -0.001318         4.36    QQQ  2023   \n",
       "2 2023-01-05  0.282380  0.038932  -0.006200         4.45    QQQ  2023   \n",
       "3 2023-01-06  0.258931  0.052242   0.000721         4.24    QQQ  2023   \n",
       "4 2023-01-09  0.267719  0.045165   0.001320         4.19    QQQ  2023   \n",
       "\n",
       "   Regime_K  \n",
       "0         2  \n",
       "1         2  \n",
       "2         2  \n",
       "3         2  \n",
       "4         2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regimes_df = pd.read_parquet(\"daily_features_with_regimes.parquet\") \n",
    "regimes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0e8aa",
   "metadata": {},
   "source": [
    "Normalizing the features so that ATM_IV, Skew, Curvature, and dgs2 are on comparable scales. This reduces bias toward large‑magnitude variables and stabilizes training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d126370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve original ATM_IV for target scaling\n",
    "regimes_df['ATM_IV_orig'] = regimes_df['ATM_IV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e10b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feature scaler\n",
    "feature_scaler_cols = ['ATM_IV','Skew','Curvature','treasury_2y']\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "regimes_df[feature_scaler_cols] = feature_scaler.fit_transform(regimes_df[feature_scaler_cols])\n",
    "\n",
    "# 2. Target scaler (ATM_IV only)\n",
    "target_scaler = StandardScaler()\n",
    "regimes_df['ATM_IV_target'] = target_scaler.fit_transform(\n",
    "    regimes_df[['ATM_IV_orig']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16eb635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ATM_IV</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Curvature</th>\n",
       "      <th>treasury_2y</th>\n",
       "      <th>ticker</th>\n",
       "      <th>year</th>\n",
       "      <th>Regime_K</th>\n",
       "      <th>ATM_IV_orig</th>\n",
       "      <th>ATM_IV_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2.424622</td>\n",
       "      <td>-0.225656</td>\n",
       "      <td>-1.169730</td>\n",
       "      <td>-0.134857</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.282223</td>\n",
       "      <td>2.424622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2.236534</td>\n",
       "      <td>0.833951</td>\n",
       "      <td>-1.341004</td>\n",
       "      <td>-0.241592</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.274456</td>\n",
       "      <td>2.236534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>2.428424</td>\n",
       "      <td>-0.886941</td>\n",
       "      <td>-2.236254</td>\n",
       "      <td>-0.001438</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.282380</td>\n",
       "      <td>2.428424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>1.860575</td>\n",
       "      <td>0.126379</td>\n",
       "      <td>-0.967097</td>\n",
       "      <td>-0.561798</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.258931</td>\n",
       "      <td>1.860575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>2.073388</td>\n",
       "      <td>-0.412409</td>\n",
       "      <td>-0.857346</td>\n",
       "      <td>-0.695216</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.267719</td>\n",
       "      <td>2.073388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    ATM_IV      Skew  Curvature  treasury_2y ticker  year  \\\n",
       "0 2023-01-03  2.424622 -0.225656  -1.169730    -0.134857    QQQ  2023   \n",
       "1 2023-01-04  2.236534  0.833951  -1.341004    -0.241592    QQQ  2023   \n",
       "2 2023-01-05  2.428424 -0.886941  -2.236254    -0.001438    QQQ  2023   \n",
       "3 2023-01-06  1.860575  0.126379  -0.967097    -0.561798    QQQ  2023   \n",
       "4 2023-01-09  2.073388 -0.412409  -0.857346    -0.695216    QQQ  2023   \n",
       "\n",
       "   Regime_K  ATM_IV_orig  ATM_IV_target  \n",
       "0         2     0.282223       2.424622  \n",
       "1         2     0.274456       2.236534  \n",
       "2         2     0.282380       2.428424  \n",
       "3         2     0.258931       1.860575  \n",
       "4         2     0.267719       2.073388  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regimes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24204e",
   "metadata": {},
   "source": [
    "Since Regime_K is a categorical state label with no natural ordering, it’s better to one‑hot encode it before feeding into the model, rather than keeping it as a raw integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e077a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode Regime_K\n",
    "regimes_df = pd.get_dummies(regimes_df, columns=['Regime_K'], prefix='Regime')\n",
    "regimes_df[['Regime_0','Regime_1','Regime_2']] = \\\n",
    "    regimes_df[['Regime_0','Regime_1','Regime_2']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12f25ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ATM_IV</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Curvature</th>\n",
       "      <th>treasury_2y</th>\n",
       "      <th>ticker</th>\n",
       "      <th>year</th>\n",
       "      <th>ATM_IV_orig</th>\n",
       "      <th>ATM_IV_target</th>\n",
       "      <th>Regime_0</th>\n",
       "      <th>Regime_1</th>\n",
       "      <th>Regime_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2.424622</td>\n",
       "      <td>-0.225656</td>\n",
       "      <td>-1.169730</td>\n",
       "      <td>-0.134857</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.282223</td>\n",
       "      <td>2.424622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2.236534</td>\n",
       "      <td>0.833951</td>\n",
       "      <td>-1.341004</td>\n",
       "      <td>-0.241592</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.274456</td>\n",
       "      <td>2.236534</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>2.428424</td>\n",
       "      <td>-0.886941</td>\n",
       "      <td>-2.236254</td>\n",
       "      <td>-0.001438</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.282380</td>\n",
       "      <td>2.428424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>1.860575</td>\n",
       "      <td>0.126379</td>\n",
       "      <td>-0.967097</td>\n",
       "      <td>-0.561798</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.258931</td>\n",
       "      <td>1.860575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>2.073388</td>\n",
       "      <td>-0.412409</td>\n",
       "      <td>-0.857346</td>\n",
       "      <td>-0.695216</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.267719</td>\n",
       "      <td>2.073388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    ATM_IV      Skew  Curvature  treasury_2y ticker  year  \\\n",
       "0 2023-01-03  2.424622 -0.225656  -1.169730    -0.134857    QQQ  2023   \n",
       "1 2023-01-04  2.236534  0.833951  -1.341004    -0.241592    QQQ  2023   \n",
       "2 2023-01-05  2.428424 -0.886941  -2.236254    -0.001438    QQQ  2023   \n",
       "3 2023-01-06  1.860575  0.126379  -0.967097    -0.561798    QQQ  2023   \n",
       "4 2023-01-09  2.073388 -0.412409  -0.857346    -0.695216    QQQ  2023   \n",
       "\n",
       "   ATM_IV_orig  ATM_IV_target  Regime_0  Regime_1  Regime_2  \n",
       "0     0.282223       2.424622         0         0         1  \n",
       "1     0.274456       2.236534         0         0         1  \n",
       "2     0.282380       2.428424         0         0         1  \n",
       "3     0.258931       1.860575         0         0         1  \n",
       "4     0.267719       2.073388         0         0         1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regimes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c0cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df, feature_cols, target_col='ATM_IV_target', W=60, H=3):\n",
    "    \"\"\"\n",
    "    Build rolling input/output sequences for multi-step forecasting.\n",
    "    \n",
    "    Args:\n",
    "        df : DataFrame with features and target\n",
    "        feature_cols : list of feature column names\n",
    "        target_col : str, column to forecast\n",
    "        W : int, input window length\n",
    "        H : int, forecast horizon (number of steps ahead)\n",
    "    \n",
    "    Returns:\n",
    "        X : np.array, shape (N, W, F)\n",
    "        y : np.array, shape (N, H)\n",
    "        meta : pd.DataFrame, metadata with ticker and end_date\n",
    "    \"\"\"\n",
    "    Xs, ys, meta = [], [], []\n",
    "    \n",
    "    for ticker, g in df.groupby('ticker'):\n",
    "        g = g.sort_values('date').reset_index(drop=True)\n",
    "        X_mat = g[feature_cols].values\n",
    "        y_mat = g[target_col].values\n",
    "        dates = g['date'].values\n",
    "        \n",
    "        for i in range(W-1, len(g)-H):\n",
    "            x_win = X_mat[i-W+1:i+1]          # shape (W, F)\n",
    "            y_target = y_mat[i+1:i+H+1]       # shape (H,)\n",
    "            \n",
    "            if np.isnan(y_target).any() or np.isnan(x_win).any():\n",
    "                continue\n",
    "            \n",
    "            Xs.append(x_win)\n",
    "            ys.append(y_target)\n",
    "            meta.append({'ticker': ticker, 'end_date': dates[i]})\n",
    "    \n",
    "    X = np.array(Xs)\n",
    "    y = np.array(ys)\n",
    "    meta = pd.DataFrame(meta)\n",
    "    return X, y, meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bbc4f",
   "metadata": {},
   "source": [
    "The input is past IV, Skew, Curvature and 2 year Treasury, while the target is future IV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b13486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 90, 7) (128, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g0/21_2mymx5mn788q92btxg65m0000gn/T/ipykernel_74415/3303809069.py:19: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for ticker, g in df.groupby('ticker'):\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['ATM_IV','Skew','Curvature','treasury_2y'] + \\\n",
    "               [c for c in regimes_df.columns if c.startswith('Regime_')]\n",
    "\n",
    "# Forecast next H days\n",
    "XH, yH, metaH = build_sequences(regimes_df, feature_cols, 'ATM_IV_target', W=90, H=14)\n",
    "print(XH.shape, yH.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2c67a",
   "metadata": {},
   "source": [
    "__Forecast using Exponential Weighted Moving Average of the past W days as baseline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6976b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma_forecast(X, y, meta, alpha=0.3):\n",
    "    \"\"\"\n",
    "    EWMA baseline: forecast next H days as the EWMA of past ATM_IV values.\n",
    "    \"\"\"\n",
    "    H = y.shape[1]\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        iv_series = x[:, 0]  # ATM_IV is first feature\n",
    "        # compute EWMA\n",
    "        ewma_val = iv_series[0]\n",
    "        for val in iv_series[1:]:\n",
    "            ewma_val = alpha * val + (1 - alpha) * ewma_val\n",
    "        preds.append(np.repeat(ewma_val, H))\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ewma = ewma_forecast(XH, yH, metaH, alpha=0.3)\n",
    "print(y_pred_ewma.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaling for both true and predicted values\n",
    "y_true_orig = target_scaler.inverse_transform(yH)\n",
    "y_pred_ewma_orig  = target_scaler.inverse_transform(y_pred_ewma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(y_true, y_pred, name=\"baseline\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))\n",
    "    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "    print(f\"{name} -> RMSE: {rmse:.4f}, MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40445e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RMSE & MAE metrics to evaluate EWMA performance between predictions and true targets:\n",
    "evaluate_baseline(y_true_orig, y_pred_ewma_orig, \"EWMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b992c6",
   "metadata": {},
   "source": [
    "Based on the above, EWMA baseline is, on average, about 1–1.5 percentage points of implied volatility away from reality. This sets the benchmark that our model needs to improve upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a7b86b",
   "metadata": {},
   "source": [
    "__Split time series to train, validation, and test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_split(X, y, meta, train_size=0.6, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Split sequences into train/val/test by time (no shuffling).\n",
    "    \n",
    "    Args:\n",
    "        X, y, meta : outputs from build_sequences\n",
    "        train_size : fraction of data for training\n",
    "        val_size   : fraction of data for validation (rest goes to test)\n",
    "    \n",
    "    Returns:\n",
    "        (X_train, y_train, meta_train),\n",
    "        (X_val, y_val, meta_val),\n",
    "        (X_test, y_test, meta_test)\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    train_end = int(N * train_size)\n",
    "    val_end   = int(N * (train_size + val_size))\n",
    "    \n",
    "    X_train, y_train, meta_train = X[:train_end], y[:train_end], meta.iloc[:train_end]\n",
    "    X_val,   y_val,   meta_val   = X[train_end:val_end], y[train_end:val_end], meta.iloc[train_end:val_end]\n",
    "    X_test,  y_test,  meta_test  = X[val_end:], y[val_end:], meta.iloc[val_end:]\n",
    "    \n",
    "    return (X_train, y_train, meta_train), (X_val, y_val, meta_val), (X_test, y_test, meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test\n",
    "(X_train, y_train, meta_train), (X_val, y_val, meta_val), (X_test, y_test, meta_test) = \\\n",
    "    time_series_split(XH, yH, metaH)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace50673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(input_shape, H, units=64, depth=1, dropout=0.2):\n",
    "    model = models.Sequential()\n",
    "    # First LSTM layer\n",
    "    model.add(layers.LSTM(units, return_sequences=(depth > 1), \n",
    "                          input_shape=input_shape))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    # Additional LSTM layers if depth > 1\n",
    "    for _ in range(depth-1):\n",
    "        model.add(layers.LSTM(units, return_sequences=False))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    # Dense output layer with H outputs (multi‑step forecast)\n",
    "    model.add(layers.Dense(H))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (XH.shape[1], XH.shape[2])  # (W, features)\n",
    "H = yH.shape[1]\n",
    "lstm_model = build_lstm(input_shape, H, units=64, depth=2, dropout=0.3)\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_scaled = lstm_model.predict(X_test)\n",
    "y_pred_orig = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true_orig = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "evaluate_baseline(y_true_orig, y_pred_orig, \"LSTM (orig units)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru(input_shape, H, units=64, depth=1, dropout=0.2):\n",
    "    model = models.Sequential()\n",
    "    # First GRU layer\n",
    "    model.add(layers.GRU(units, return_sequences=(depth > 1), \n",
    "                         input_shape=input_shape))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    # Additional GRU layers if depth > 1\n",
    "    for _ in range(depth-1):\n",
    "        model.add(layers.GRU(units, return_sequences=False))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    # Dense output layer with H outputs (multi‑step forecast)\n",
    "    model.add(layers.Dense(H))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (XH.shape[1], XH.shape[2])  # (W, features)\n",
    "H = yH.shape[1]\n",
    "gru_model = build_gru(input_shape, H, units=64, depth=2, dropout=0.3)\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_scaled = gru_model.predict(X_test)\n",
    "y_pred_orig = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true_orig = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "evaluate_baseline(y_true_orig, y_pred_orig, \"GRU (orig units)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cd686",
   "metadata": {},
   "source": [
    "__Hyperparameter Tuning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp_or_dict, H=3, input_shape=None):\n",
    "    \n",
    "    # If it's a dict, just read values directly\n",
    "    if isinstance(hp_or_dict, dict):\n",
    "        units   = hp_or_dict['units']\n",
    "        depth   = hp_or_dict['depth']\n",
    "        dropout = hp_or_dict['dropout']\n",
    "        cell    = hp_or_dict['cell']\n",
    "    else:\n",
    "        # Otherwise assume it's a KerasTuner HyperParameters object\n",
    "        units   = hp_or_dict.Int('units', min_value=32, max_value=128, step=32)\n",
    "        depth   = hp_or_dict.Int('depth', 1, 3)\n",
    "        dropout = hp_or_dict.Float('dropout', 0.1, 0.5, step=0.1)\n",
    "        cell    = hp_or_dict.Choice('cell', ['LSTM', 'GRU'])\n",
    "        \n",
    "    model = models.Sequential()\n",
    "    for i in range(depth):\n",
    "        return_seq = (i < depth - 1)\n",
    "        if cell == 'LSTM':\n",
    "            model.add(layers.LSTM(units, return_sequences=return_seq,\n",
    "                                  input_shape=input_shape if i == 0 else None))\n",
    "        else:\n",
    "            model.add(layers.GRU(units, return_sequences=return_seq,\n",
    "                                 input_shape=input_shape if i == 0 else None))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    model.add(layers.Dense(H))  # multi‑step output\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_results = []\n",
    "\n",
    "for W in [90]:\n",
    "    for H in [14]:\n",
    "        X, y, meta = build_sequences(regimes_df, feature_cols, 'ATM_IV_target', W=W, H=H)\n",
    "        (X_train, y_train, meta_train), (X_val, y_val, meta_val), (X_test, y_test, meta_test) = time_series_split(X, y, meta)\n",
    "\n",
    "        tuner = kt.RandomSearch(\n",
    "            lambda hp: model_builder(hp, H=H),\n",
    "            objective='val_loss',\n",
    "            max_trials=10,\n",
    "            directory='tuner_dir',\n",
    "            project_name=f'iv_forecast_W{W}_H{H}',\n",
    "            overwrite=True\n",
    "        )\n",
    "        tuner.search(X_train, y_train,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     epochs=30,\n",
    "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "        \n",
    "        best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "        print(f\"W={W}, H={H}, best config:\", best_hp.values)\n",
    "        best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "        best_val_loss = best_trial.metrics.get_last_value(\"val_loss\")\n",
    "        \n",
    "        tuner_results.append({\n",
    "            \"W\": W,\n",
    "            \"H\": H,\n",
    "            \"best_hp\": best_hp.values,\n",
    "            \"val_loss\": best_val_loss\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad82b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_df = pd.DataFrame(tuner_results)\n",
    "print(tuner_df.sort_values(\"val_loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67519fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = tuner_df.sort_values(\"val_loss\").iloc[0]\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_best = best_row[\"W\"]\n",
    "H_best = best_row[\"H\"]\n",
    "\n",
    "X, y, meta = build_sequences(regimes_df, feature_cols, 'ATM_IV_target', W=W_best, H=H_best)\n",
    "(X_train, y_train, meta_train), (X_val, y_val, meta_val), (X_test, y_test, meta_test) = time_series_split(X, y, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078899b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_hp_values = best_row[\"best_hp\"]  \n",
    "best_model = model_builder(best_hp_values, H_best, input_shape=X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90721554",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae976c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred_scaled = best_model.predict(X_test)\n",
    "\n",
    "# Invert scaling\n",
    "y_pred_orig = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true_orig = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_baseline(y_true_orig, y_pred_orig, \"Best tuned model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_builder(hp_or_dict, H=3, input_shape=None):\n",
    "    # Handle dict vs HyperParameters\n",
    "    if isinstance(hp_or_dict, dict):\n",
    "        units   = hp_or_dict['units']\n",
    "        dropout = hp_or_dict['dropout']\n",
    "        cell    = hp_or_dict['cell']\n",
    "    else:\n",
    "        units   = hp_or_dict.Int('units', min_value=32, max_value=128, step=32)\n",
    "        dropout = hp_or_dict.Float('dropout', 0.1, 0.5, step=0.1)\n",
    "        cell    = hp_or_dict.Choice('cell', ['LSTM', 'GRU'])\n",
    "\n",
    "    # Encoder: produce context vector\n",
    "    encoder_inputs = tf.keras.Input(shape=input_shape, name=\"encoder_inputs\")\n",
    "    if cell == 'LSTM':\n",
    "        context = tf.keras.layers.LSTM(units, dropout=dropout, name=\"encoder_lstm\")(encoder_inputs)\n",
    "    else:\n",
    "        context = tf.keras.layers.GRU(units, dropout=dropout, name=\"encoder_gru\")(encoder_inputs)\n",
    "\n",
    "    # Repeat context H times to form decoder input sequence\n",
    "    repeated = tf.keras.layers.RepeatVector(int(H), name=\"repeat_vector\")(context)\n",
    "\n",
    "    # Decoder: generate H-step sequence\n",
    "    if cell == 'LSTM':\n",
    "        decoder_seq = tf.keras.layers.LSTM(units, return_sequences=True, dropout=dropout, name=\"decoder_lstm\")(repeated)\n",
    "    else:\n",
    "        decoder_seq = tf.keras.layers.GRU(units, return_sequences=True, dropout=dropout, name=\"decoder_gru\")(repeated)\n",
    "\n",
    "    outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1), name=\"decoder_dense\")(decoder_seq)\n",
    "\n",
    "    model = tf.keras.Model(encoder_inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_tuner_results = []\n",
    "\n",
    "for W in [90]:\n",
    "    for H in [14]:\n",
    "        X, y, meta = build_sequences(regimes_df, feature_cols, 'ATM_IV_target', W=W, H=H)\n",
    "        (X_train, y_train, meta_train), (X_val, y_val, meta_val), (X_test, y_test, meta_test) = time_series_split(X, y, meta)\n",
    "\n",
    "        input_shape = X_train.shape[1:]  # (W, num_features)\n",
    "\n",
    "        tuner = kt.RandomSearch(\n",
    "            lambda hp: seq2seq_builder(hp, H=H, input_shape=input_shape),\n",
    "            objective='val_loss',\n",
    "            max_trials=10,\n",
    "            directory='tuner_dir',\n",
    "            project_name=f'seq2seq_W{W}_H{H}',\n",
    "            overwrite=True\n",
    "        )\n",
    "\n",
    "        # IMPORTANT: Seq2Seq expects [encoder_inputs, decoder_inputs]\n",
    "        # For a baseline, you can feed zeros as decoder_inputs during training\n",
    "        decoder_train = np.zeros((X_train.shape[0], H, 1))\n",
    "        decoder_val   = np.zeros((X_val.shape[0], H, 1))\n",
    "        \n",
    "        tuner.search(X_train, y_train,\n",
    "             validation_data=(X_val, y_val),\n",
    "             epochs=30,\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "\n",
    "#         tuner.search([X_train, decoder_train], y_train,\n",
    "#                      validation_data=([X_val, decoder_val], y_val),\n",
    "#                      epochs=30,\n",
    "#                      callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "\n",
    "        best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "        best_trial = tuner.oracle.get_best_trials(1)[0]\n",
    "        best_val_loss = best_trial.metrics.get_last_value(\"val_loss\")\n",
    "\n",
    "        print(f\"W={W}, H={H}, best config:\", best_hp.values)\n",
    "\n",
    "        seq_tuner_results.append({\n",
    "            \"W\": W,\n",
    "            \"H\": H,\n",
    "            \"best_hp\": best_hp.values,\n",
    "            \"val_loss\": best_val_loss\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_tuner_df = pd.DataFrame(seq_tuner_results)\n",
    "print(seq_tuner_df.sort_values(\"val_loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe80cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = seq_tuner_df.sort_values(\"val_loss\").iloc[0]\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b43506",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_best = best_row[\"W\"]\n",
    "H_best = best_row[\"H\"]\n",
    "\n",
    "X, y, meta = build_sequences(regimes_df, feature_cols, 'ATM_IV_target', W=W_best, H=H_best)\n",
    "(X_train, y_train, meta_train), (X_val, y_val, meta_val), (X_test, y_test, meta_test) = time_series_split(X, y, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ade9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp_values = best_row[\"best_hp\"] \n",
    "\n",
    "input_shape = X_train.shape[1:]       # (W, num_features)\n",
    "best_model = seq2seq_builder(best_hp_values, H_best, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694da121",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=30,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "# Flatten last two dims into (num_samples, H)\n",
    "y_pred_scaled_2d = y_pred_scaled.reshape(y_pred_scaled.shape[0], -1)\n",
    "y_test_2d        = y_test.reshape(y_test.shape[0], -1)\n",
    "\n",
    "# Inverse transform\n",
    "y_pred_orig_2d = target_scaler.inverse_transform(y_pred_scaled_2d)\n",
    "y_true_orig_2d = target_scaler.inverse_transform(y_test_2d)\n",
    "\n",
    "# Optionally reshape back to (num_samples, H, 1)\n",
    "y_pred_orig = y_pred_orig_2d.reshape(y_pred_scaled.shape)\n",
    "y_true_orig = y_true_orig_2d.reshape(y_test.shape)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_baseline(y_true_orig, y_pred_orig, \"Best tuned model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce176f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
